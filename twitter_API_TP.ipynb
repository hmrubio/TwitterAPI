{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Trabajo Práctico - Twitter API<center>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "En éste trabajo presentamos una pequeña aplicación encargada de la recolección y presentación de información proporcionada por la **API** de **Twitter**. Ésta misma buscará aquellos tweets que nos resulten interesantes según la temáctica que hayamos elegido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query\n",
    "A la hora de elegir un contenido para la investigación nos tomamos el trabajo de seleccionar un temática concurrente para el desarrollo de una consulta firme y robusta.\n",
    "Decidimos utilisar el tópico de cambio climático, y sus temas relacionados, para la construcción de la siguiente QUERY.\n",
    "\n",
    "***QUERY = \"cambio climático OR sequías OR calentamiento global OR economía circular OR espacios verde OR protección ambiental lang:es -is:retweet\"***\n",
    "\n",
    "Con ésta misma anduvimos recolecantdo una gran cantidad de tweets para el desarrollo de una estrucutura compuesta para el procesamiento, lectura y servicio de los datos almacenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recolección de Tweets\n",
    "\n",
    "Con el siguiente código pudimos recolectar una cierta cantidad de Tweets. Modificamos el mismo, proporcionado por las presentaciones dadas en clase, para que sea más optimo para nuestra tarea. Incluimos una limpieza de las ***STREAM RULES***, esto mismo lo vimos importante debido a que, cada vez que realizabamos una nueva busqueda, estas mismas quedaban con la información de la busqueda previa. Como consecuencia, nos quedaba \"información basura\" para futuras consultas. También incluimos la hora en la que se inicia la búsqueda, la cantidad de Tweets recopilados hasta el momento, el tamaño del archivo, como también la datos de la duracióon de la prueba, entre otros datos que nos parecieron importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from TwitterAPI import (\n",
    "    TwitterAPI,\n",
    "    TwitterOAuth,\n",
    "    TwitterRequestError,\n",
    "    TwitterConnectionError,\n",
    "    HydrateType,\n",
    "    OAuthType,\n",
    ")\n",
    "import json\n",
    "\n",
    "def stream_tweets(query, expansions, tweet_fields, user_fields):\n",
    "\n",
    "    datetimestart = datetime\n",
    "    try:\n",
    "        o = TwitterOAuth.read_file(\"credentials.txt\")\n",
    "        api = TwitterAPI(\n",
    "            o.consumer_key,\n",
    "            o.consumer_secret,\n",
    "            auth_type=OAuthType.OAUTH2,\n",
    "            api_version=\"2\",\n",
    "        )\n",
    "\n",
    "        # DELETE STREAM RULES\n",
    "        r = api.request(\"tweets/search/stream/rules\", method_override=\"GET\")\n",
    "        rules = r.json()\n",
    "        if \"data\" in rules:\n",
    "            ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "            api.request(\"tweets/search/stream/rules\", {\"delete\": {\"ids\": ids}})\n",
    "\n",
    "        # ADD STREAM RULES\n",
    "        r = api.request(\"tweets/search/stream/rules\", {\"add\": [{\"value\": query}]})\n",
    "        print(f\"[{r.status_code}] RULE ADDED: {json.dumps(r.json(), indent=2)}\\n\")\n",
    "        if r.status_code != 201:\n",
    "            exit()\n",
    "\n",
    "        # GET STREAM RULES\n",
    "\n",
    "        r = api.request(\"tweets/search/stream/rules\", method_override=\"GET\")\n",
    "        print(f\"[{r.status_code}] RULES: {json.dumps(r.json(), indent=2)}\\n\")\n",
    "        if r.status_code != 200:\n",
    "            exit()\n",
    "\n",
    "        # START STREAM\n",
    "\n",
    "        r = api.request(\n",
    "            \"tweets/search/stream\",\n",
    "            {\n",
    "                \"expansions\": expansions,\n",
    "                \"tweet.fields\": tweet_fields,\n",
    "                \"user.fields\": user_fields,\n",
    "            },\n",
    "            hydrate_type=HydrateType.APPEND,\n",
    "        )\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            exit()\n",
    "        \n",
    "        if not os.path.exists(\"data.json\"):\n",
    "            open(\"data.json\", \"x\", encoding=\"utf-8\")\n",
    "\n",
    "        with open(\"data.json\", \"r+\", encoding=\"utf-8\") as file:\n",
    "            cantidad_tweets = len(file.readlines())\n",
    "            file.seek(0, os.SEEK_END)\n",
    "\n",
    "            print(\"------------------------------------------------------------\")\n",
    "            datetimestart = datetime.now()\n",
    "            print(\n",
    "                (\n",
    "                    \"Proceso de recopilación iniciado: \"\n",
    "                    + datetimestart.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for item in r:\n",
    "                json.dump(item, file, ensure_ascii=False, indent=None)\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "                cantidad_tweets += 1\n",
    "                sys.stdout.write(\n",
    "                    f\"\\rTamaño actual del archivo: {file.tell() / 1000} kb | Cantidad de tweets: {cantidad_tweets}\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        datetimeend = datetime.now()\n",
    "        print(\"\\nProceso terminado: \" + datetimeend.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        datetimeend = datetimeend - datetimestart\n",
    "        print(\"Duración de la prueba \" + (str(datetimeend)) + \" horas/minutos/segundos\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "    except TwitterRequestError as e:\n",
    "        print(f\"\\n{e.status_code}\")\n",
    "        for msg in iter(e):\n",
    "            print(msg)\n",
    "    except TwitterConnectionError as e:\n",
    "        print(e)\n",
    "        print(\"tce\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"e\")\n",
    "\n",
    "\n",
    "QUERY = \"cambio climático OR sequías OR calentamiento global OR economía circular OR espacios verde OR protección ambiental lang:es -is:retweet\"\n",
    "EXPANSIONS = \"author_id,referenced_tweets.id,referenced_tweets.id.author_id,in_reply_to_user_id,attachments.media_keys,attachments.poll_ids,geo.place_id,entities.mentions.username\"\n",
    "TWEET_FIELDS = \"author_id,conversation_id,created_at,entities,geo,id,lang,public_metrics,source,text\"\n",
    "USER_FIELDS = \"created_at,description,entities,location,name,profile_image_url,public_metrics,url,username\"\n",
    "\n",
    "stream_tweets(QUERY, EXPANSIONS, TWEET_FIELDS, USER_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta por fechas y horas\n",
    "\n",
    "En ésta sección decidimos utilizar la estructura de un ***INDICE INVERTIDO***. Elegimos utilizar como ***KEY*** el usuario creador de cada tweet y, el número de línea donde se encuentra el tweet, como su correspondiente conjunto.\n",
    "\n",
    "Cuando se realiza una consulta, si se propociona el usuario, se realiza una búsquerda dentro del mismo. De todos los tweets obtenidos, filtramos aquellos que concuerden con aquellas fechas proporcionadas por la consulta.\n",
    "\n",
    "En el caso que no se proporcione ninguna fecha, utilizamos un rango mínimo-máximo de esta misma para poder continuar con la búsqueda.\n",
    "\n",
    "En el caso que no se proporcione un usuario, se realiza una búsqueda lineal debido a la imposibilidad de hacerlo con una ***BINARY SEARCH***\n",
    "\n",
    "(La siguiente imagen es a modo de ilustración y breve explicación del funcionamiento de un ***INDICE INVERTIDO*** )\n",
    "\n",
    "![alt text](inverted_index.png \"Indice Invertido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from nltk.stem import SnowballStemmer  # Stemmer\n",
    "from nltk.corpus import stopwords  # Stopwords\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "class CreacionDeBloques:\n",
    "    def __init__(self, documento, salida, temp=\"./temp\", language='spanish'):\n",
    "        ''' documentos: carpeta con archivos a indexar\n",
    "            salida: carpeta donde se guardará el índice invertido'''\n",
    "        self.documento = documento\n",
    "        self.salida = salida\n",
    "        self._blocksize = 5000\n",
    "        self._temp = temp\n",
    "        self._stop_words = frozenset(stopwords.words(language))  # lista de stop words\n",
    "        self._stemmer = SnowballStemmer(language, ignore_stopwords=False)\n",
    "        self._term_to_termID = {}\n",
    "        self._user_to_userID = {}\n",
    "\n",
    "        self.__indexar()\n",
    "\n",
    "    def __lematizar(self, palabra):\n",
    "        ''' Usa el stemmer para lematizar o recortar la palabra, previamente elimina todos\n",
    "        los signos de puntuación que pueden aparecer. El stemmer utilizado también se\n",
    "        encarga de eliminar acentos y pasar todo a minúscula, sino habría que hacerlo\n",
    "        a mano'''\n",
    "\n",
    "        # palabra = palabra.decode(\"utf-8\", ignore).encode(\"utf-8\")\n",
    "        palabra = palabra.strip(string.punctuation + \"»\" + \"\\x97\" + \"¿\" + \"¡\" + \"\\u201c\" + \\\n",
    "                                \"\\u201d\" + \"\\u2014\" + \"\\u2014l\" + \"\\u00bf\")\n",
    "        # \"\\x97\" representa un guión\n",
    "\n",
    "        palabra_lematizada = self._stemmer.stem(palabra)\n",
    "        return palabra_lematizada\n",
    "\n",
    "    def __indexar(self):\n",
    "        n = 0\n",
    "        lista_bloques_palabras = []\n",
    "        lista_bloques_usuarios= []\n",
    "        for bloque_palabras,bloque_usuarios in self.__parse_next_block():\n",
    "            bloque_invertido_palabras = self.__invertir_bloque(\n",
    "                bloque_palabras)            # ahora cada bloque tine cada palabra con todos los tweets de esa palabra\n",
    "\n",
    "            bloque_invertido_usuarios = self.__invertir_bloque(\n",
    "                bloque_usuarios)            # ahora cada bloque tine cada usuario con todos los tweets de esa usuario\n",
    "\n",
    "            lista_bloques_palabras.append(self.__guardar_bloque_intermedio(bloque_invertido_palabras, n))\n",
    "            lista_bloques_usuarios.append(self.__guardar_bloque_intermedio(bloque_invertido_usuarios, f\"u{n}\"))\n",
    "            n += 1\n",
    "        start = time.process_time()\n",
    "        self.__intercalar_bloques(lista_bloques_palabras, self._term_to_termID, \"postings\")\n",
    "        self.__intercalar_bloques(lista_bloques_usuarios,self._user_to_userID,\"user_postings\")\n",
    "        end = time.process_time()\n",
    "        print(\"Intercalar Bloques Elapsed time: \", end - start)\n",
    "\n",
    "        self.__guardar_diccionario_terminos(self._term_to_termID, \"terminos\")\n",
    "        self.__guardar_diccionario_terminos(self._user_to_userID, \"usuarios\")\n",
    "\n",
    "    def __invertir_bloque(self, bloque):\n",
    "        bloque_invertido = {}\n",
    "        bloque_ordenado = sorted(bloque, key=lambda tupla: (tupla[0], tupla[1]))\n",
    "        for par in bloque_ordenado:\n",
    "            posting = bloque_invertido.setdefault(par[0], set())\n",
    "            posting.add(par[1])\n",
    "        return bloque_invertido\n",
    "\n",
    "    def __guardar_bloque_intermedio(self, bloque, nro_bloque):\n",
    "        archivo_salida = \"b\" + str(nro_bloque) + \".json\"\n",
    "        archivo_salida = os.path.join(self._temp, archivo_salida)\n",
    "        for clave in bloque:\n",
    "            bloque[clave] = list(bloque[clave])\n",
    "        with open(archivo_salida, \"w+\") as contenedor:\n",
    "            json.dump(bloque, contenedor)\n",
    "        return archivo_salida\n",
    "\n",
    "    def __intercalar_bloques(self, temp_files, term_to_termID, nombre_archivo_salida):\n",
    "\n",
    "        lista_termID = [str(value) for value in term_to_termID.values()]\n",
    "        iter_lista = iter(lista_termID)\n",
    "        cantidad_term_group = len(lista_termID) // 1000 + 1\n",
    "\n",
    "        posting_file = os.path.join(self.salida, f\"{nombre_archivo_salida}.json\")\n",
    "        open_files = [open(f, \"r\") for f in temp_files]\n",
    "\n",
    "        with open(posting_file, \"w+\") as salida:\n",
    "\n",
    "            for x in range(cantidad_term_group):\n",
    "                postings = {}\n",
    "                lista_parte_term_ID = [next(iter_lista, None) for i in range(1000)]\n",
    "\n",
    "                for data in open_files:\n",
    "                    data.seek(0)\n",
    "                    bloque = json.load(data)\n",
    "\n",
    "                    i = 0\n",
    "                    while i < 1000 and lista_parte_term_ID[i]:\n",
    "                        try:\n",
    "                            postings[i] = postings.setdefault(i,set()).union(set(bloque[lista_parte_term_ID[i]]))\n",
    "                        except:\n",
    "                            pass\n",
    "                        i += 1\n",
    "                for posting in postings.values():\n",
    "                    json.dump(list(posting), salida, indent=None)\n",
    "                    salida.write('\\n')\n",
    "\n",
    "    def __guardar_diccionario_terminos(self, term_to_termID, nombre_archivo_diccionario):\n",
    "        path = os.path.join(self.salida, f\"diccionario_{nombre_archivo_diccionario}.json\")\n",
    "        with open(path, \"w\") as contenedor:\n",
    "            for term, termID in term_to_termID.items():\n",
    "                json.dump((term, termID), contenedor)\n",
    "                contenedor.write(\"\\n\")\n",
    "\n",
    "    def __parse_next_block(self):\n",
    "        n = self._blocksize  # espacio libre en el bloque actual\n",
    "        termID = 1  # inicializamos el diccionario de términos\n",
    "        userID = 1  # inicializamos el diccionario de términos\n",
    "        bloque_palabras = []  # lista de pares (termID, tweetID)\n",
    "        bloque_usuarios = []\n",
    "\n",
    "        tweetID = 1  # ID de cada tweet, se puede acceder directament desde el json\n",
    "        with open(self.documento, encoding=\"utf-8\") as file:\n",
    "            for tweet in file:\n",
    "                n -= 1\n",
    "                #Recorro las palabras\n",
    "                palabras = json.loads(tweet)['data']['text'].split()  # va palabra por palabra del tweet\n",
    "                for pal in palabras:\n",
    "                    if pal not in self._stop_words:\n",
    "                        pal = self.__lematizar(pal)\n",
    "                        if pal not in self._term_to_termID:\n",
    "                            self._term_to_termID[pal] = termID\n",
    "                            termID += 1\n",
    "                        bloque_palabras.append((self._term_to_termID[pal], tweetID))\n",
    "\n",
    "                #Recorro los usuarios\n",
    "                usuario = json.loads(tweet)[\"data\"][\"author_id_hydrate\"][\"username\"]\n",
    "                if usuario not in self._user_to_userID:\n",
    "                    self._user_to_userID[usuario] = userID\n",
    "                    userID += 1\n",
    "                bloque_usuarios.append((self._user_to_userID[usuario], tweetID))\n",
    "\n",
    "                tweetID += 1\n",
    "                if n <= 0:\n",
    "                    yield(bloque_palabras,bloque_usuarios)\n",
    "                    n = self._blocksize\n",
    "                    bloque_palabras = []\n",
    "                    bloque_usuarios = []\n",
    "            yield(bloque_palabras,bloque_usuarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta por palabras\n",
    "\n",
    "Al seleccionar la opción de consultar por palabras solicitamos la cantidad de tweets (m número de los mismos) y la consulta a realizar. Manejamos que el número de tweets sea mayor que cero, de lo contrario, lanzamos una ValueError Exception, con el mensaje de que la cantidad de tweets no es válida.\n",
    "\n",
    "A la hora de realizar la consulta analizamos aquellos operadores proporcionados. Estos mismos son válidos si son ***OR - AND - NOT*** y también que la query esté completa. Si la misma no tiene un buen formato, levantamos una Exception propia (***BadQueryFormat***)\n",
    "\n",
    "Si todas las validaciones previas son correctas, se ejecuta la consulta solicitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _buscar_palabra(palabra):\n",
    "    if (palabra != \"*\"):\n",
    "        palabra = _lematizar(palabra.strip('\"'))\n",
    "        with open(\"./output/diccionario_terminos.json\", \"r\") as contenedor:\n",
    "            linea = next(contenedor, False)\n",
    "            while (linea):\n",
    "                linea = json.loads(linea)\n",
    "                if (linea[0] == palabra):\n",
    "                    break\n",
    "                else:\n",
    "                    linea = next(contenedor, False)\n",
    "    else: linea = \"Comodin\"\n",
    "\n",
    "    conjunto = set()\n",
    "    if (linea == \"Comodin\"):\n",
    "        with open(\"./output/postings.json\", \"r\") as contenedor:\n",
    "            while(contenedor):\n",
    "                try: conjunto.update(json.loads(next(contenedor)))\n",
    "                except StopIteration: break\n",
    "    elif (linea):\n",
    "        with open(\"./output/postings.json\", \"r\") as contenedor:\n",
    "            for i in range(1, linea[1]):\n",
    "                valor = next(contenedor)\n",
    "            conjunto.update(json.loads(next(contenedor)))\n",
    "\n",
    "    return conjunto\n",
    "\n",
    "def _buscar_palabras(query):\n",
    "    matches = re.findall(r'\\([^()]+\\)|\\\"(?:[^\\\"]+)\\\"|and not|and|not|or', query)\n",
    "\n",
    "    if (len(matches) % 2 == 0): \n",
    "        if (matches[0] == \"not\"):\n",
    "            matches[0] = \"and not\"\n",
    "            matches.insert(0, \"*\")\n",
    "        else: \n",
    "            raise BadQueryFormat(\"Falta un operador que vincule dos términos: \" + query)\n",
    "\n",
    "    out = set()\n",
    "    for i in range(0, len(matches), 2):\n",
    "        if matches[i][0] == \"(\":\n",
    "            conjunto = _buscar_palabras(matches[i].strip(\"()\"))\n",
    "        elif (type(matches[i]) != type(set)):\n",
    "            texto = re.findall(r'\\S+', matches[i])\n",
    "\n",
    "            conjunto = set()\n",
    "            if (len(texto) > 1):\n",
    "                for palabra in texto:\n",
    "                    conjunto.update(_buscar_palabra(palabra))\n",
    "            else:\n",
    "                conjunto = _buscar_palabra(matches[i])\n",
    "\n",
    "        operador = \"\"\n",
    "        if ((i - 1) > 0):\n",
    "            operador = matches[i - 1]\n",
    "        elif (i == 0):\n",
    "            out.update(conjunto)\n",
    "\n",
    "        if (operador == \"and\"):\n",
    "            out.intersection_update(conjunto)\n",
    "        elif (operador == \"or\"):\n",
    "            out.update(conjunto)\n",
    "        elif (operador == \"and not\" or operador == \"not\"):\n",
    "            out.difference_update(conjunto)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación de Tweets\n",
    "\n",
    "En éste caso requerimos los nombres de:\n",
    "\n",
    "1. rimer archivo\n",
    "2. Segundo archivo\n",
    "3. Archivo resultante\n",
    "\n",
    "(Estos mismos deben estar con la extensión ***.json***)\n",
    "\n",
    "Manejamos que los nombres proporcionados cumplan con los requerimientos solicitados previamente, caso contrario, levantamos una ValueError Exception.\n",
    "\n",
    "Si todas las comprobaciones cumplen con lo esperado, realizamos la agrupación/unión correspondiente de los archivos.\n",
    "\n",
    "Este proceso comienza con la búsqueda y comprobación previa de los archivos presentados previamente. Realizamos una obtención de tweets y sus correspondientes fechas. AL tener toda la información corespondiente, analizamos que cada sección cumpla con la existencia de una fecha y comparaciones entre las mismas para la correcta inserción de la información en el nuevo archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_tweets_ordenados(file1, file2, salida):\n",
    "\n",
    "    if not(os.path.exists(file1) and os.path.exists(file2)):\n",
    "        print(\"Los archivos no existen\")\n",
    "        return\n",
    "    with open(file1, \"r\", encoding=\"utf-8\") as archivo_tweets1, open(file2, \"r\", encoding=\"utf-8\") as archivo_tweets2, open(salida, \"w\", encoding=\"utf-8\") as conjunto_tweets:\n",
    "\n",
    "        tweet_archivo1, fecha_tweet1 = _obtener_tweet_y_fecha(archivo_tweets1)\n",
    "        tweet_archivo2, fecha_tweet2 = _obtener_tweet_y_fecha(archivo_tweets2)\n",
    "\n",
    "        while(tweet_archivo1 or tweet_archivo2):\n",
    "                if(not fecha_tweet1):\n",
    "                    json.dump(tweet_archivo2, conjunto_tweets,ensure_ascii=False, indent=None)\n",
    "                    tweet_archivo2, fecha_tweet2 = _obtener_tweet_y_fecha(archivo_tweets2)\n",
    "\n",
    "                elif(not fecha_tweet2):\n",
    "                    json.dump(tweet_archivo1, conjunto_tweets,ensure_ascii=False, indent=None)\n",
    "                    tweet_archivo1, fecha_tweet1 = _obtener_tweet_y_fecha(archivo_tweets1)\n",
    "\n",
    "                elif(fecha_tweet1 < fecha_tweet2):\n",
    "                    json.dump(tweet_archivo1, conjunto_tweets,ensure_ascii=False, indent=None)\n",
    "                    tweet_archivo1, fecha_tweet1 = _obtener_tweet_y_fecha(archivo_tweets1)\n",
    "\n",
    "                else:\n",
    "                    json.dump(tweet_archivo2, conjunto_tweets,ensure_ascii=False, indent=None)\n",
    "                    tweet_archivo2, fecha_tweet2 = _obtener_tweet_y_fecha(archivo_tweets2)\n",
    "                conjunto_tweets.write(\"\\n\")\n",
    "\n",
    "def _obtener_tweet_y_fecha(archivo_tweet):\n",
    "    tweet = next(archivo_tweet, None)\n",
    "    fecha = None\n",
    "    while(tweet and not fecha):\n",
    "        try:\n",
    "            tweet = json.loads(tweet)\n",
    "            fecha = datetime.strptime(tweet[\"data\"][\"created_at\"],\n",
    "                                     '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        except KeyError:\n",
    "            print(\"KeyError\")\n",
    "            tweet = next(archivo_tweet, None)\n",
    "    return(tweet, fecha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
