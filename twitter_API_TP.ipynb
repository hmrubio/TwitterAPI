{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Trabajo Práctico - Twitter API<center>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "En éste trabajo presentamos una pequeña aplicación encargada de la recolección y presentación de información proporcionada por la **API** de **Twitter**. Ésta misma buscará aquellos tweets que nos resulten interesantes según la temáctica que hayamos elegido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query\n",
    "A la hora de elegir un contenido para la investigación nos tomamos el trabajo de seleccionar un temática concurrente para el desarrollo de una consulta firme y robusta.\n",
    "Decidimos utilisar el tópico de cambio climático, y sus temas relacionados, para la construcción de la siguiente QUERY.\n",
    "\n",
    "***QUERY = \"cambio climático OR sequías OR calentamiento global OR economía circular OR espacios verde OR protección ambiental lang:es -is:retweet\"***\n",
    "\n",
    "Con ésta misma anduvimos recolecantdo una gran cantidad de tweets para el desarrollo de una estrucutura compuesta para el procesamiento, lectura y servicio de los datos almacenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recolección de Tweets\n",
    "\n",
    "Con el siguiente código pudimos recolectar una cierta cantidad de Tweets. Modificamos el mismo, proporcionado por las presentaciones dadas en clase, para que sea más optimo para nuestra tarea. Incluimos una limpieza de las ***STREAM RULES***, esto mismo lo vimos importante debido a que, cada vez que realizabamos una nueva busqueda, estas mismas quedaban con la información de la busqueda previa. Como consecuencia, nos quedaba \"información basura\" para futuras consultas. También incluimos la hora en la que se inicia la búsqueda, la cantidad de Tweets recopilados hasta el momento, el tamaño del archivo, como también la datos de la duracióon de la prueba, entre otros datos que nos parecieron importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from TwitterAPI import (\n",
    "    TwitterAPI,\n",
    "    TwitterOAuth,\n",
    "    TwitterRequestError,\n",
    "    TwitterConnectionError,\n",
    "    HydrateType,\n",
    "    OAuthType,\n",
    ")\n",
    "import json\n",
    "\n",
    "def stream_tweets(query, expansions, tweet_fields, user_fields):\n",
    "\n",
    "    datetimestart = datetime\n",
    "    try:\n",
    "        o = TwitterOAuth.read_file(\"credentials.txt\")\n",
    "        api = TwitterAPI(\n",
    "            o.consumer_key,\n",
    "            o.consumer_secret,\n",
    "            auth_type=OAuthType.OAUTH2,\n",
    "            api_version=\"2\",\n",
    "        )\n",
    "\n",
    "        # DELETE STREAM RULES\n",
    "        r = api.request(\"tweets/search/stream/rules\", method_override=\"GET\")\n",
    "        rules = r.json()\n",
    "        if \"data\" in rules:\n",
    "            ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "            api.request(\"tweets/search/stream/rules\", {\"delete\": {\"ids\": ids}})\n",
    "\n",
    "        # ADD STREAM RULES\n",
    "        r = api.request(\"tweets/search/stream/rules\", {\"add\": [{\"value\": query}]})\n",
    "        print(f\"[{r.status_code}] RULE ADDED: {json.dumps(r.json(), indent=2)}\\n\")\n",
    "        if r.status_code != 201:\n",
    "            exit()\n",
    "\n",
    "        # GET STREAM RULES\n",
    "\n",
    "        r = api.request(\"tweets/search/stream/rules\", method_override=\"GET\")\n",
    "        print(f\"[{r.status_code}] RULES: {json.dumps(r.json(), indent=2)}\\n\")\n",
    "        if r.status_code != 200:\n",
    "            exit()\n",
    "\n",
    "        # START STREAM\n",
    "\n",
    "        r = api.request(\n",
    "            \"tweets/search/stream\",\n",
    "            {\n",
    "                \"expansions\": expansions,\n",
    "                \"tweet.fields\": tweet_fields,\n",
    "                \"user.fields\": user_fields,\n",
    "            },\n",
    "            hydrate_type=HydrateType.APPEND,\n",
    "        )\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            exit()\n",
    "        \n",
    "        if not os.path.exists(\"data.json\"):\n",
    "            open(\"data.json\", \"x\", encoding=\"utf-8\")\n",
    "\n",
    "        with open(\"data.json\", \"r+\", encoding=\"utf-8\") as file:\n",
    "            cantidad_tweets = len(file.readlines())\n",
    "            file.seek(0, os.SEEK_END)\n",
    "\n",
    "            print(\"------------------------------------------------------------\")\n",
    "            datetimestart = datetime.now()\n",
    "            print(\n",
    "                (\n",
    "                    \"Proceso de recopilación iniciado: \"\n",
    "                    + datetimestart.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for item in r:\n",
    "                json.dump(item, file, ensure_ascii=False, indent=None)\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "                cantidad_tweets += 1\n",
    "                sys.stdout.write(\n",
    "                    f\"\\rTamaño actual del archivo: {file.tell() / 1000} kb | Cantidad de tweets: {cantidad_tweets}\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        datetimeend = datetime.now()\n",
    "        print(\"\\nProceso terminado: \" + datetimeend.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        datetimeend = datetimeend - datetimestart\n",
    "        print(\"Duración de la prueba \" + (str(datetimeend)) + \" horas/minutos/segundos\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "\n",
    "    except TwitterRequestError as e:\n",
    "        print(f\"\\n{e.status_code}\")\n",
    "        for msg in iter(e):\n",
    "            print(msg)\n",
    "    except TwitterConnectionError as e:\n",
    "        print(e)\n",
    "        print(\"tce\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"e\")\n",
    "\n",
    "\n",
    "QUERY = \"cambio climático OR sequías OR calentamiento global OR economía circular OR espacios verde OR protección ambiental lang:es -is:retweet\"\n",
    "EXPANSIONS = \"author_id,referenced_tweets.id,referenced_tweets.id.author_id,in_reply_to_user_id,attachments.media_keys,attachments.poll_ids,geo.place_id,entities.mentions.username\"\n",
    "TWEET_FIELDS = \"author_id,conversation_id,created_at,entities,geo,id,lang,public_metrics,source,text\"\n",
    "USER_FIELDS = \"created_at,description,entities,location,name,profile_image_url,public_metrics,url,username\"\n",
    "\n",
    "stream_tweets(QUERY, EXPANSIONS, TWEET_FIELDS, USER_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta por fechas y horas\n",
    "\n",
    "En ésta sección decidimos utilizar la estructura de ***B-Tree***\n",
    "\n",
    "![alt text](b-tree.webp \"B-Tree\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
